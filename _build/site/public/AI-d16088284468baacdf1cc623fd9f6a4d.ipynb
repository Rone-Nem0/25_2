{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0adc60",
   "metadata": {},
   "source": [
    "# Artificial Inteligence \n",
    "\n",
    "# Introduction of AI\n",
    "Explain the main types of thecniques and the secuence of this in the course:\n",
    "1. Neural Networks training\n",
    "2. Difuse Logic using mapping to realize FIS (Fuzzy inference system)\n",
    "![alt text](AI/Ai.jpeg)\n",
    "\n",
    "State Stacionary and state Transient.\n",
    "\n",
    "\n",
    "## History of the trough hardness of Artificial Inteligence\n",
    "\n",
    "\n",
    "1. Resolve the XOR, about one of the most hardness challenges in the 19 century, this make the silence advanced in the field for the 60's\n",
    "\n",
    "\n",
    "## Biological Neuron  \n",
    "The inspiration of the Artificial Inteligence with the biological neurons and the parts of this try to make a rebuild-ing\n",
    "- **Dentrita:** Is the ramification of the core of the neurons joint with a lot of anothers neurons with one make transfer of the signal electric.\n",
    "- **Cinapsis:** is the operation of the transfer of information in this phase the weight of exitation is  \n",
    "- Axon\n",
    "\n",
    "## Convert and use it this meaningfull core function of neurons:\n",
    "\n",
    "- **Input:** Signals appear to make actions over the neuronsOutput Umbral (thetha).\n",
    "- **Core - Umbral (theta):** this have the rules to be exited in this the action is getting.\n",
    "- **Output:** Is the response of the neuron to the exitation.\n",
    "\n",
    "**In the neuron have the pre-post learning geriano (Donald ger).**\n",
    "\n",
    "ENTRY -> WEIGHT -> ZUMATORY -> UMBRAL -> OUTPUT\n",
    "\n",
    "## Basic Elements of Artificial Neuron\n",
    "\n",
    "1. Input\n",
    "2. Weight\n",
    "3. Sumatory \n",
    "4. Transfer function\n",
    "5. Output\n",
    "\n",
    "## The first techniques \n",
    "In this drawing understand theta like is a threshold \n",
    "\n",
    "```{math}\n",
    ":label: eq-theta\n",
    "\\theta = x_0 \\times w_0\n",
    "```\n",
    "\n",
    "## Evolution of Artificial Neuron Models\n",
    "\n",
    "### 1943: Pioneering Model\n",
    "\n",
    "In this technique, the input to the function is $V$ (the sum of weighted inputs), and the threshold ($\\theta$) defines the neuron's activation. The output is determined by the function:\n",
    "\n",
    "$$\n",
    "Y = f(V - \\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "**Flow:**  \n",
    "2 Inputs → Weights → Summation ($V$) → Activation Function ($f(V - \\theta)$) → Output\n",
    "\n",
    "---\n",
    "\n",
    "### 1950: Modernized Model\n",
    "\n",
    "In this model, the threshold ($\\theta$) is treated as an additional input (often called the \"bias\" or \"incognite\"). The activation depends on the sum $V$:\n",
    "\n",
    "$$\n",
    "Y = f(V)\n",
    "$$\n",
    "\n",
    "\n",
    "**Flow:**  \n",
    "2 Inputs + 1 Bias (Incognite) → Summation ($V$) → Activation Function ($f(V)$) → Output\n",
    "\n",
    "### The Perceptron in 1962\n",
    "\n",
    "The perceptron introduced a mathematical framework for classifying inputs based on weighted sums and a threshold (bias). The perceptron computes:\n",
    "\n",
    "$$\n",
    "X_0 W_0 + X_1 W_1 + X_2 W_2 = X \\cdot W + b\n",
    "$$\n",
    "\n",
    "where $X$ is the input vector, $W$ is the weight vector, and $b$ is the bias.\n",
    "\n",
    "#### Example: The AND Function\n",
    "\n",
    "Consider the dataset $Q$ for the logical AND operation:\n",
    "\n",
    "```{code-block} python\n",
    "Q = [\n",
    "    ([0, 0], 0),\n",
    "    ([0, 1], 0),\n",
    "    ([1, 0], 0),\n",
    "    ([1, 1], 1)\n",
    "]\n",
    "```\n",
    "\n",
    "Here, $[x_1, x_2]$ are the inputs and $y$ is the desired output.\n",
    "\n",
    "#### Decision Boundary\n",
    "\n",
    "Suppose we choose weights $W_1 = -1$, $W_2 = 1$, and bias $b = 1$. The perceptron decision boundary is:\n",
    "\n",
    "$$\n",
    "X \\cdot W + b = 0\n",
    "$$\n",
    "\n",
    "or, in terms of $x_1$ and $x_2$:\n",
    "\n",
    "$$\n",
    "x_2 = x_1 - 1\n",
    "$$\n",
    "\n",
    "This equation defines a straight line in the $(x_1, x_2)$ plane, separating the input space into two regions:\n",
    "\n",
    "- **Positive region:** $X \\cdot W + b > 0$\n",
    "- **Negative region:** $X \\cdot W + b < 0$\n",
    "\n",
    "For example, when $x_1 = 0$ and $x_2 = 0$:\n",
    "\n",
    "$$\n",
    "X \\cdot W + b = 0 \\cdot (-1) + 0 \\cdot 1 + 1 = 1\n",
    "$$\n",
    "\n",
    "This point lies in the positive region. The perceptron thus separates the input space into two classes using a linear boundary.\n",
    "\n",
    "```{note}\n",
    "The perceptron can only solve problems that are linearly separable.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b17951",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Supervised Learning\n",
    "\n",
    "## Types of Data \n",
    "\n",
    "1. **Data binary:** The values used are **0** and **1**.\n",
    "2. **Data binary bipolar:** The values used are **-1** and **1**\n",
    "\n",
    "Knowing the data is expresed like: Q = { (X,T)} == {(P,T)} be X = [x1, x2] , T = Y.\n",
    "\n",
    "![alt text](AI/bias_perceptron.png)\n",
    "![alt text](AI/theta.png)\n",
    "\n",
    "\n",
    "\n",
    "The perceptron update rule is given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta (y - \\hat{y}) \\mathbf{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b \\leftarrow b + \\eta (y - \\hat{y})\n",
    "$$\n",
    "\n",
    "where  \n",
    "- $\\mathbf{w}$: weight vector  \n",
    "- $\\mathbf{x}$: input vector  \n",
    "- $y$: true label  \n",
    "- $\\hat{y}$: predicted label  \n",
    "- $\\eta$: learning rate  \n",
    "- $b$: bias\n",
    "The perceptron make the funcion be a heavyside.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initial perceptron parameters for interactive adjustment in JupyterLite\n",
    "\n",
    "# import numpy as np\n",
    "# from IPython.display import display\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # AND logic gate data\n",
    "# X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "# y = np.array([0, 0, 0, 1])\n",
    "\n",
    "\n",
    "# # Perceptron parameters\n",
    "# w = [0.1,0.1]\n",
    "# b = 0\n",
    "# lr = 0.5\n",
    "# epochs = 5\n",
    "\n",
    "# # For plotting decision boundary\n",
    "# def plot_decision_boundary(X, y, w, b, epoch):\n",
    "#     plt.clf()\n",
    "#     for i, label in enumerate(y):\n",
    "#         plt.scatter(X[i,0], X[i,1], c='red' if label==1 else 'blue', s=100)\n",
    "#     x_vals = np.array([0,1])\n",
    "#     if w[1] != 0:\n",
    "#         y_vals = -(w[0]*x_vals + b)/w[1]\n",
    "#         plt.plot(x_vals, y_vals, 'k--')\n",
    "#     plt.xlim(-0.2,1.2)\n",
    "#     plt.ylim(-0.2,1.2)\n",
    "#     plt.title(f'Perceptron Training - Epoch {epoch}')\n",
    "#     plt.xlabel('x1')\n",
    "#     plt.ylabel('x2')\n",
    "#     plt.grid(True)\n",
    "#     plt.pause(0.01)\n",
    "\n",
    "# plt.figure(figsize=(6,6))\n",
    "# for epoch in range(epochs):\n",
    "#     plot_decision_boundary(X, y, w, b, epoch)\n",
    "#     for i in range(len(X)):\n",
    "#         z = np.dot(X[i], w) + b\n",
    "#         y_pred = 1 if z > 0 else 0\n",
    "#         w += lr * (y[i] - y_pred) * X[i]\n",
    "#         b += lr * (y[i] - y_pred)\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfb08e",
   "metadata": {},
   "source": [
    "## The bipolar Perceptron\n",
    "The bipolar perceptron uses inputs and outputs in the set $\\{-1, +1\\}$ instead of $\\{0, 1\\}$. The summation function computes $v = \\mathbf{w} \\cdot \\mathbf{x} + b$, and the activation is the sign function:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(v) =\n",
    "\\begin{cases}\n",
    "+1 & \\text{if } v \\geq 0 \\\\\n",
    "-1 & \\text{if } v < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This allows the perceptron to model logic gates and classification tasks where the classes are represented as $-1$ and $+1$. The update rule remains similar, but with bipolar targets and predictions.\n",
    "![alt text](AI/Sign_Function.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a7f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q ipywidgets\n",
    "# import numpy as np\n",
    "# from ipywidgets import interact, FloatSlider, IntSlider # <-- FIX: Import widget classes\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Data and function definitions as before...\n",
    "# X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "# y = np.array([0, 0, 0, 1])\n",
    "\n",
    "# def perceptron_interactive(w0_slider, w1_slider, b_slider, lr_slider, epochs_slider):\n",
    "\n",
    "#     w = np.array([w0_slider, w1_slider])\n",
    "#     bias = b_slider\n",
    "#     lr = lr_slider\n",
    "#     epochs = epochs_slider\n",
    "\n",
    "\n",
    "#     # w = [0.1,0.1]\n",
    "#     # bias = 0\n",
    "#     # lr = 0.5\n",
    "#     # epochs =5\n",
    "#     plt.figure(figsize=(6,6))\n",
    "#     for epoch in range(epochs):\n",
    "#         plt.clf()\n",
    "#         Plot point\n",
    "#         for i, label in enumerate(y):\n",
    "#             plt.scatter(X[i,0], X[i,1], c='red' if label==1 else 'blue', s=100)\n",
    "#         # Plot decision boundary\n",
    "#         x_vals = np.array([0,1])\n",
    "#         if w[1] != 0:\n",
    "#             y_vals = -(w[0]*x_vals + bias)/w[1]\n",
    "#             plt.plot(x_vals, y_vals, 'k--')\n",
    "        \n",
    "#         plt.xlim(-0.2,1.2)\n",
    "#         plt.ylim(-0.2,1.2)\n",
    "#         plt.title(f'Epoch {epoch}')\n",
    "#         plt.xlabel('x1')\n",
    "#         plt.ylabel('x2')\n",
    "#         plt.grid(True)\n",
    "#         plt.pause(0.01)\n",
    "\n",
    "#         # Perceptron update\n",
    "#         for i in range(len(X)):\n",
    "#             z = np.dot(X[i], w) + bias\n",
    "#             y_pred = 1 if z > 0 else 0\n",
    "#             w += lr * (y[i] - y_pred) * X[i]\n",
    "#             bias += lr * (y[i] - y_pred)\n",
    "\n",
    "# interact(\n",
    "#     perceptron_interactive,\n",
    "#     w0_slider=FloatSlider(min=0, max=1, step=0.1, value=0.1, description=\"w0\"),\n",
    "#     w1_slider=FloatSlider(min=0, max=1, step=0.1, value=0.1, description=\"w1\"),\n",
    "#     b_slider=FloatSlider(min=0, max=1, step=0.1, value=0, description=\"bias\"),\n",
    "#     lr_slider=FloatSlider(min=0, max=1, step=0.1, value=0.1, description=\"lr\"),\n",
    "#     epochs_slider=IntSlider(min=1, max=10, step=1, value=5, description=\"epochs\")\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9555729",
   "metadata": {},
   "source": [
    "The equation of the perceptron decision boundary is:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\cdot \\mathbf{w} + b = 0\n",
    "$$\n",
    "\n",
    "For two features ($x_1$, $x_2$) and weights ($w_0$, $w_1$):\n",
    "\n",
    "$$\n",
    "x_1 w_0 + x_2 w_1 + b = 0\n",
    "$$\n",
    "\n",
    "To express $x_2$ in terms of $x_1$, $w_0$, $w_1$, and $b$:\n",
    "\n",
    "$$\n",
    "x_2 = -\\frac{w_0}{w_1} x_1 - \\frac{b}{w_1}\n",
    "$$\n",
    "\n",
    "**Summary of perceptron equations:**\n",
    "\n",
    "- **Weighted sum:** $z = \\mathbf{x} \\cdot \\mathbf{w} + b$\n",
    "- **Activation (binary):** $\\hat{y} = \\begin{cases} 1 & \\text{if } z > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$\n",
    "- **Activation (bipolar):** $\\hat{y} = \\text{sign}(z)$\n",
    "- **Update rule:** $\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta (y - \\hat{y}) \\mathbf{x}$ and $b \\leftarrow b + \\eta (y - \\hat{y})$\n",
    "\n",
    "## The validation\n",
    "\n",
    "To validate a perceptron, we train it on labeled data and observe how well it classifies points in the input space. For two features, this means plotting the data points and the decision boundary in the 2D plane. The perceptron learns to separate the classes by adjusting its weights and bias so that the activation function produces the correct output for each input.\n",
    "\n",
    "## Desired data\n",
    "\n",
    "The target (desired output) is directly related to the activation function:\n",
    "- With a **step function** (Heaviside), outputs are binary ($0$ or $1$).\n",
    "- With a **sign function**, outputs are bipolar ($-1$ or $+1$).\n",
    "The bias $b$ shifts the activation threshold, allowing the decision boundary to move within the input space.\n",
    "\n",
    "## Function of the Learning\n",
    "\n",
    "**Topology of the Perceptron:**  \n",
    "- Input data $X$ has shape $(m, n)$, where $m$ is the number of samples and $n$ is the number of features (dimensions).\n",
    "- Weight vector $\\mathbf{w}$ has shape $(n,)$.\n",
    "- The activation function computes $z = \\mathbf{x} \\cdot \\mathbf{w} + b$ for each input $\\mathbf{x}$.\n",
    "\n",
    "**Learning rule:**\n",
    "1. If the output $a$ matches the target $t$ ($a = t$), the error is zero and the weights remain unchanged.\n",
    "2. If the output is incorrect, the update is $\\Delta w_j = \\eta (t - a) x_j$. This means:\n",
    "    - If $x_j = 0$, $w_j$ does not change.\n",
    "    - If $x_j = 1$, $w_j$ is increased or decreased depending on the sign of $(t - a)$.\n",
    "    - The bias is updated similarly: $b \\leftarrow b + \\eta (t - a)$.\n",
    "\n",
    "This process iteratively adjusts the weights and bias to minimize classification errors, shaping the decision boundary to best separate the classes in the $n$-dimensional feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d062203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q ipywidgets\n",
    "# import numpy as np\n",
    "# from IPython.display import Markdown, display\n",
    "# import math\n",
    "# from ipywidgets import interact, FloatSlider, IntSlider # <-- FIX: Import widget classes\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Data and function definitions as before...\n",
    "# X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "\n",
    "# def perceptron_interactive(random_slider, y0_slider, y1_slider, y2_slider, y3_slider, w0_slider, w1_slider, b_slider, lr_slider, epochs_slider):\n",
    "\n",
    "#     display(Markdown(\"If you want a Random Value of the weights and the bias put the random value = 1!\")),\n",
    "#     if random_slider == 1:\n",
    "#         w0_slider = np.random.rand()\n",
    "#         w1_slider = np.random.rand()\n",
    "#         b_slider = np.random.rand()\n",
    "#         display(Markdown(f\"Randomly selected weights: w0={w0_slider:.2f}, w1={w1_slider:.2f}, bias={b_slider:.2f}\"))\n",
    "\n",
    "#     y = np.array([y0_slider, y1_slider, y2_slider, y3_slider])\n",
    "#     w = np.array([w0_slider, w1_slider])\n",
    "#     bias = b_slider\n",
    "#     lr = lr_slider\n",
    "#     epochs = epochs_slider\n",
    "\n",
    "\n",
    "#     # w = [0.1,0.1]\n",
    "#     # bias = 0\n",
    "#     # lr = 0.5\n",
    "#     # epochs =5\n",
    "#     plt.figure(figsize=(5,5))\n",
    "#     for epoch in range(epochs):\n",
    "#         plt.clf()\n",
    "#         # Plot points\n",
    "#         for i, label in enumerate(y):\n",
    "#             plt.scatter(X[i,0], X[i,1], c='red' if label==1 else 'blue', s=100)\n",
    "#         # Plot decision boundary\n",
    "#         x_vals = np.array([-0.2,1.2])\n",
    "#         if w[1] != 0:\n",
    "#             y_vals = -(w[0]*x_vals + bias)/w[1]\n",
    "#             plt.plot(x_vals, y_vals, 'k--')\n",
    "        \n",
    "#         plt.xlim(-0.2,1.2)\n",
    "#         plt.ylim(-0.2,1.2)\n",
    "#         plt.title(f'Epoch {epoch}')\n",
    "#         plt.xlabel('x1')\n",
    "#         plt.ylabel('x2')\n",
    "#         plt.grid(True)\n",
    "#         plt.pause(0.01)\n",
    "#         if w[1] != 0:\n",
    "#             angle_rad = math.atan(-w[0]/w[1])\n",
    "#             angle_deg = math.degrees(angle_rad)\n",
    "            \n",
    "#             display(Markdown(f\" The slop: m={-w[0]/w[1]:.2f},angle={angle_deg:.2f}, short_in_y={-bias/w[1]:.2f}\"))\n",
    "        \n",
    "#         if w[1] == 0:\n",
    "#             display(Markdown(f\" Doesn't exist the hiperplane to separate in 2 class\"))\n",
    "            \n",
    "\n",
    "\n",
    "#         # Perceptron update\n",
    "#         for i in range(len(X)):\n",
    "#             z = np.dot(X[i], w) + bias\n",
    "#             y_pred = 1 if z > 0 else 0\n",
    "#             w += lr * (y[i] - y_pred) * X[i]\n",
    "#             bias += lr * (y[i] - y_pred)\n",
    "#             display(Markdown(f\" weights in each iteration: w0={w[0]:.2f}, w1={w[1]:.2f}, bias={bias:.2f}\"))\n",
    "\n",
    "\n",
    "# display(Markdown(\n",
    "#                 \"Este es un proceso interactivo de entrenamiento de un perceptrón binario.\"\n",
    "#                 \"Puedes ajustar manualmente los pesos, el sesgo, la tasa de aprendizaje, las etiquetas de salida y el número de épocas, \"\n",
    "#                 \"o seleccionar valores aleatorios para los pesos y el sesgo.<br>\"\n",
    "#                 \"El perceptrón se entrena sobre datos tipo Q = { (P, T) } donde P ∈ {0, 1}² y T ∈ {0, 1}.<br>\"\n",
    "#                 \"Observa cómo evoluciona la frontera de decisión durante el entrenamiento.\"\n",
    "#             ))\n",
    "# interact(\n",
    "    \n",
    "#     perceptron_interactive,\n",
    "#     random_slider=IntSlider(min=0, max=1, step=1, value=0, description=\"Random\"),\n",
    "#     y0_slider=IntSlider(min=0, max=1, step=1, value=0, description=\"y(X=[0,0]\"),\n",
    "#     y1_slider=IntSlider(min=0, max=1, step=1, value=0, description=\"y(X=[0,1]\"),\n",
    "#     y2_slider=IntSlider(min=0, max=1, step=1, value=0, description=\"y(X=[1,0]\"),\n",
    "#     y3_slider=IntSlider(min=0, max=1, step=1, value=1, description=\"y(X=[1,1]\"),\n",
    "#     w0_slider=FloatSlider(min=0, max=1, step=0.1, value=0.1, description=\"w0\"),\n",
    "#     w1_slider=FloatSlider(min=0, max=1, step=0.1, value=0.1, description=\"w1\"),\n",
    "#     b_slider=FloatSlider(min=0, max=1, step=0.1, value=0, description=\"bias\"),\n",
    "#     lr_slider=FloatSlider(min=0, max=1, step=0.1, value=0.1, description=\"lr\"),\n",
    "#     epochs_slider=IntSlider(min=1, max=20, step=1, value=5, description=\"epochs\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928de63",
   "metadata": {},
   "source": [
    "## Analysis of the Perceptron Weight Update Rule\n",
    "\n",
    "The perceptron weight update rule is:\n",
    "\n",
    "$$\n",
    "w_j \\leftarrow w_j + \\eta \\cdot (Y[i] - \\hat{y}) \\cdot X[i]_j\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $w_j$ is the $j$-th weight,\n",
    "- $\\eta$ is the learning rate,\n",
    "- $Y[i]$ is the true label,\n",
    "- $\\hat{y}$ is the predicted label,\n",
    "- $X[i]_j$ is the $j$-th feature of the $i$-th input.\n",
    "\n",
    "### Case Analysis\n",
    "\n",
    "#### 1. When $X[i]_j = 0$:\n",
    "- The update becomes $w_j \\leftarrow w_j$ (no change).\n",
    "- **Interpretation:** If feature $j$ is not present in the input, its weight is unaffected.\n",
    "\n",
    "#### 2. When $X[i]_j = 1$:\n",
    "- The update depends on $(Y[i] - \\hat{y})$:\n",
    "\n",
    "    - **If prediction is correct** ($Y[i] = \\hat{y}$):  \n",
    "        - $(Y[i] - \\hat{y}) = 0$  \n",
    "        - No change to $w_j$.\n",
    "\n",
    "    - **If $Y[i] = 1$, $\\hat{y} = 0$** (under-prediction):  \n",
    "        - $(Y[i] - \\hat{y}) = 1$  \n",
    "        - $w_j$ increases by $\\eta$.\n",
    "\n",
    "    - **If $Y[i] = 0$, $\\hat{y} = 1$** (over-prediction):  \n",
    "        - $(Y[i] - \\hat{y}) = -1$  \n",
    "        - $w_j$ decreases by $\\eta$.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Correct predictions:** No weight change.\n",
    "- **Under-prediction:** Increase weights for active features ($X[i]_j = 1$).\n",
    "- **Over-prediction:** Decrease weights for active features.\n",
    "\n",
    "This mechanism ensures that only the weights corresponding to active features are adjusted, and only when the prediction is incorrect. The learning rate $\\eta$ controls the step size of each update, allowing the perceptron to iteratively reduce prediction errors.\n",
    "\n",
    "\n",
    "\\begin {align*} w_j &:= w_j + \\Delta w_j \\\\ & := w_j + \\eta (y_i - \\hat y_i)x_{ij} \\\\ &= \\begin{cases} w_j &\\text{(a) } y_i - \\hat y_i = 0\\\\ w_j + \\eta x_ij &\\text{(b) } y_i - \\hat y_i = 1 \\\\ w_j - \\eta x_ij &\\text{(c) } y_i - \\hat y_i = -1 \\\\ \\end{cases} \\end{align*}\n",
    "\n",
    "\\begin {align*} b &:= b + \\Delta b \\\\ & := b + \\eta (y_i - \\hat y_i) \\\\ &= \\begin{cases} b &\\text{(a) } y_i - \\hat y_i = 0\\\\ b + \\eta &\\text{(b) } y_i - \\hat y_i = 1 \\\\ b - \\eta &\\text{(c) } y_i - \\hat y_i = -1 \\\\ \\end{cases} \\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q ipywidgets\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "import math\n",
    "from ipywidgets import interact, FloatSlider, IntSlider # <-- FIX: Import widget classes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data and function definitions as before...\n",
    "X = np.array([[1,-1], [-1,-1], [1,1], [-1,1]])\n",
    "\n",
    "def perceptron_interactive(random_slider, y0_slider, y1_slider, y2_slider, y3_slider, w0_slider, w1_slider, b_slider, lr_slider, epochs_slider):\n",
    "\n",
    "    display(Markdown(\"If you want a Random Value of the weights and the bias put the random value = 1!\")),\n",
    "    if random_slider == 1:\n",
    "        w0_slider = np.random.rand()\n",
    "        w1_slider = np.random.rand()\n",
    "        b_slider = np.random.rand()\n",
    "        display(Markdown(f\"Randomly selected weights: w0={w0_slider:.2f}, w1={w1_slider:.2f}, bias={b_slider:.2f}\"))\n",
    "\n",
    "    y = np.array([y0_slider, y1_slider, y2_slider, y3_slider])\n",
    "    w = np.array([w0_slider, w1_slider])\n",
    "    bias = b_slider\n",
    "    lr = lr_slider\n",
    "    epochs = epochs_slider\n",
    "\n",
    "\n",
    "    # w = [0.1,0.1]\n",
    "    # bias = 0\n",
    "    # lr = 0.5\n",
    "    # epochs =5\n",
    "    plt.figure(figsize=(6,6))\n",
    "    for epoch in range(epochs):\n",
    "        plt.clf()\n",
    "        # Plot points\n",
    "        for i, label in enumerate(y): \n",
    "\n",
    "            plt.scatter(X[i,0], X[i,1], c='red' if label==1 else 'blue', s=100)\n",
    "        # Plot decision boundary\n",
    "        x_vals = np.array([-3,3])\n",
    "\n",
    "        if w[1] != 0:\n",
    "            y_vals = -(w[0]*x_vals - bias)/w[1]\n",
    "            plt.plot(x_vals, y_vals, 'k--')\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        plt.xlim(-3,3)\n",
    "        plt.ylim(-3,3)\n",
    "        plt.title(f'Epoch {epoch}')\n",
    "        plt.xlabel('x1')\n",
    "        plt.ylabel('x2')\n",
    "        plt.grid(True)\n",
    "        plt.pause(0.01)\n",
    "        if w[1] != 0:\n",
    "            angle_rad = math.atan(-w[0]/w[1])\n",
    "            angle_deg = math.degrees(angle_rad)\n",
    "            \n",
    "            display(Markdown(f\" The slop: m={-w[0]/w[1]:.2f},angle={angle_deg:.2f}, short_in_y={-bias/w[1]:.2f}\"))\n",
    "        \n",
    "        if w[1] == 0:\n",
    "            display(Markdown(f\" Doesn't exist the hiperplane to separate in 2 class\"))\n",
    "            \n",
    "\n",
    "\n",
    "        # Perceptron update\n",
    "        for i in range(len(X)):\n",
    "            z = np.dot(X[i], w) - bias\n",
    "            y_pred = 1 if z >= 0 else -1\n",
    "            w += lr * (y[i] - y_pred) * X[i]\n",
    "            bias -= lr * (y[i] - y_pred)\n",
    "            display(Markdown(f\" weights in each iteration: w0={w[0]:.2f}, w1={w[1]:.2f}, bias={bias:.2f}\"))\n",
    "\n",
    "\n",
    "display(Markdown(\n",
    "                \"Este es un proceso interactivo de entrenamiento de un perceptrón binario.\"\n",
    "                \"Puedes ajustar manualmente los pesos, el sesgo, la tasa de aprendizaje, las etiquetas de salida y el número de épocas, \"\n",
    "                \"o seleccionar valores aleatorios para los pesos y el sesgo.<br>\"\n",
    "                \"El perceptrón se entrena sobre datos tipo Q = { (P, T) } donde P ∈ {0, 1}² y T ∈ {0, 1}.<br>\"\n",
    "                \"Observa cómo evoluciona la frontera de decisión durante el entrenamiento.\"\n",
    "            ))\n",
    "interact(\n",
    "    \n",
    "    perceptron_interactive,\n",
    "    random_slider=IntSlider(min=0, max=1, step=1, value=0, description=\"Random\"),\n",
    "    y0_slider=IntSlider(min=-1, max=1, step=1, value=-1, description=\"y(X=[1,-1]\"),\n",
    "    y1_slider=IntSlider(min=-1, max=1, step=1, value=-1, description=\"y(X=[-1,-1]\"),\n",
    "    y2_slider=IntSlider(min=-1, max=1, step=1, value=1, description=\"y(X=[1,1]\"),\n",
    "    y3_slider=IntSlider(min=-1, max=1, step=1, value=-1, description=\"y(X=[-1,1]\"),\n",
    "    w0_slider=FloatSlider(min=-1, max=1, step=0.1, value=0.3, description=\"w0\"),\n",
    "    w1_slider=FloatSlider(min=-1, max=1, step=0.1, value=-0.2, description=\"w1\"),\n",
    "    b_slider=FloatSlider(min=-1, max=1, step=0.1, value=-0.6, description=\"bias\"),\n",
    "    lr_slider=FloatSlider(min=0, max=1, step=0.1, value=0.5, description=\"lr\"),\n",
    "    epochs_slider=IntSlider(min=1, max=20, step=1, value=5, description=\"epochs\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
